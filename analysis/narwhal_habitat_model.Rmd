---
title: "Narwhal habitat model"
author: "Marie Zahn"
date: '2022-12-12'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'C:/Users/marie/Documents/OMG_Narwhals/OMG_narwhals_habitat-model')
```

```{r load packages}
library(tidyverse)
library(corrplot)
library(MuMIn)
library(arm)
library(faraway)
library(ggplot2)
library(sjPlot)
library(here)
library(car)
library(ggpubr)
library(raincloudplots)
```

## Open Data

```{r open rdata files}
# reload data from anywhere
# data from Aug-Oct
load(here("data-cleaning_R/narwhal_model_data_aug-oct.rdata")) 
```

## Summarize narwhal detections

```{r narwhal detection totals}
## pool soundtrap and aural detections for 2019 data (i.e., take only one obs/day for each site)
mdl_data_pooled_2019 <- mdl_data %>% group_by(site,time) %>% 
  filter(year==2019, narwhal==max(narwhal),(row_number()==n()))
## pool detections for entire dataset (i.e., pool 2019 but retain 2018 detections)
mdl_data_pooled <- mdl_data %>% group_by(site,time) %>% 
  filter(narwhal==max(narwhal),(row_number()==n())) %>% dplyr::select(-device)

## identify how many detections from the same day occurred at more than one site
length(unique(mdl_data_pooled$time)) # number of days (obs) for open water period
sum(mdl_data_pooled$narwhal) # total of 93 detections over two years at 3 sites (92 not counting outlier)
narwhal_dets <- mdl_data_pooled %>% filter(narwhal==1) # number of days that narwhals were detected
length(unique(narwhal_dets$time)) # number of unique days when narwhals were identified (77 including outlier)

## determine how many days whales were seen at more than one glacier front
mult_det <- mdl_data_pooled %>% dplyr::select(time, site, narwhal) %>% pivot_wider(names_from = site, values_from = narwhal) %>% mutate(rowsums = sum(kong,rink,sver,na.rm=TRUE)) %>% filter(rowsums>1)
length(mult_det$time)

## determine how many days whales were seen at more than one glacier front each year
doubles_2018 <- mult_det %>% filter(grepl("2018",time))
doubles_2019 <- mult_det %>% filter(grepl("2019",time))
length(doubles_2018$time)
length(doubles_2019$time)

## tally up total detections for each site for a given year
rink_2018 <- mdl_data_pooled %>% filter(year==2018, site=='rink')
rink_2019 <- mdl_data_pooled %>% filter(year==2019, site=='rink')
sum(rink_2018$narwhal)
sum(rink_2019$narwhal)+1 # (add outlier that was removed)

kong_2018 <- mdl_data_pooled %>% filter(year==2018, site=='kong')
kong_2019 <- mdl_data_pooled %>% filter(year==2019, site=='kong')
sum(kong_2018$narwhal)
sum(kong_2019$narwhal)

sver_2018 <- mdl_data_pooled %>% filter(year==2018, site=='sver')
sum(sver_2018$narwhal)

## extract last detection for each year
mdl_data_pooled_2018 <- mdl_data_pooled %>% filter(year==2018, narwhal==1)
mdl_data_pooled_2019 <- mdl_data_pooled %>% filter(year==2019, narwhal==1)
tail(mdl_data_pooled_2018)
tail(mdl_data_pooled_2019)
```

Out of the 77 days that narwhals were detected, 16 of those days were occasions when whales were detected at two locations on the same day. Whales were never detected at three locations on the same day in 2018.

Totals between each year/site

2018
* Rink: 13
* Kong Oscar: 8
* Sverdrup: 7

2019
* Rink: 32
* Kong Oscar: 33

## Compare detections between Aurals and SoundTraps in 2019

```{r aural vs soundtrap 2019}
## see whether there are any aural detections that were NOT detected by the soundtrap in 2019
aural_2019 <- mdl_data %>% group_by(site) %>% filter(year==2019,device=='aural') %>% dplyr::select(time,narwhal) %>% rename(narwhal_aural=narwhal)
sound_2019 <- mdl_data %>% group_by(site) %>%  filter(year==2019,device=='soundtrap') %>% dplyr::select(time,narwhal)%>% rename(narwhal_sound=narwhal)

compare <- aural_2019 %>% group_by(site) %>%  left_join(sound_2019,by="time") %>% filter(site.x==site.y, narwhal_aural==1 & narwhal_sound==0)
compare # zero rows = no aural detections that were not picked up by soundtrap

## calculate the reverse - how many soundtrap detections were not matched in aurals
compare <- sound_2019 %>% group_by(site) %>%  left_join(aural_2019,by="time") %>% filter(site.x==site.y, narwhal_sound==1 & narwhal_aural==0)
compare

## check sums across years
sum(mdl_data_pooled$narwhal)
mdl_data %>% filter(year==2018) %>% dplyr::select(narwhal) %>% sum()
mdl_data %>% filter(year==2019,device=='soundtrap') %>% dplyr::select(narwhal) %>% sum()

## double check comparison using different method
aural_2019_kong <- aural_2019 %>% filter(site=='kong') 
sound_2019_kong <- sound_2019 %>% filter(site=='kong') 
aural_2019_rink <- aural_2019 %>% filter(site=='rink') 
sound_2019_rink <- sound_2019 %>% filter(site=='rink') 

aural_2019_kong %>% left_join(sound_2019_kong,by="time") %>% filter(narwhal_aural==1&narwhal_sound==0)
aural_2019_rink %>% left_join(sound_2019_rink,by="time") %>% filter(narwhal_aural==1&narwhal_sound==0)

```

## Normalize glacier length data

```{r normalize glacier length data}
## normalize at the start of each year
ref <- mdl_data_pooled %>% group_by(year, site) %>% filter(time==min(as.Date(time))) %>% dplyr::select(site,year,glacier_length) %>% dplyr::rename(length_ref=glacier_length)

## add normalized glacier length variable and drop unneeded variables
mdl_data_clean <- mdl_data_pooled %>% left_join(ref,by=c("site","year")) %>% mutate(glacier_len_norm = glacier_length - length_ref) %>% dplyr::select(!c(length_ref,noise_100Hz,noise_2kHz,runoff_mar,glacier_length))

## sanity check plot
mdl_data_clean %>% ggplot(aes(x=as.Date(time),y=glacier_len_norm,color=site))+
  geom_point()

## make sure data are correct
mdl_data %>% filter(time=='2019-09-01')
mdl_data_clean %>% filter(time=='2019-09-01')
  
```

## Check correlations

```{r check correlations}
## check correlations now that glacier length has been normalized
## Aug-Oct period
mdl_data_corr <- mdl_data_clean %>% ungroup() %>% dplyr::select(!c(narwhal, time, site, year)) %>% cor(use = "complete.obs")
## plot correlation test
corrplot::corrplot(mdl_data_corr, method = 'number')

## Hide upper triangle and export csv of correlations
mdl_data_tbl <- mdl_data_clean %>% ungroup() %>% dplyr::select(!c(narwhal, time, site, year)) %>% cor(use = "complete.obs") %>% round(2) %>% as.data.frame()

mdl_data_tbl[upper.tri(mdl_data_tbl)]<-""
mdl_data_tbl<-as.data.frame(mdl_data_tbl)
write.csv(mdl_data_tbl, here("analysis/csv_outputs/mdl_data_correlations.csv"),row.names = TRUE)
```

## Exploratory plots

```{r raincloud plots}
variables <- c("DOY","runoff_racmo","salt_shallow","salt_deep","temp_shallow","temp_deep",
               "ice_cover_percent","velocity","noise_4kHz","glacier_len_norm")

raincloud_list <- list()

for (i in 1:length(variables)){
  data_1 <- mdl_data_clean %>% filter(narwhal==1) %>% dplyr::select(variables[i])
  data_2 <- mdl_data_clean %>% filter(narwhal==0) %>% dplyr::select(variables[i])
  
  # make dataframe
  df_1x1 <- data_1x1(
    array_1 = unlist(array(data_1[variables[i]])),
    array_2 = unlist(array(data_2[variables[i]])),
    jit_distance = .09,
    jit_seed = 321)
  
  # plot
  raincloud_2 <- raincloud_1x1_repmes(
    data = df_1x1,
    colors = (c('dodgerblue', 'darkorange')),
    fills = (c('dodgerblue', 'darkorange')),
    line_color = 'white',
    line_alpha = .3,
    size = 1,
    alpha = .6,
    align_clouds = FALSE) +
   
  scale_x_continuous(breaks=c(1,2), labels=c("1", "0"), limits=c(0, 3)) +
    xlab("Narwhal") + 
    ylab(variables[i]) +
    theme_classic()
  
  raincloud_list[[i]] <- raincloud_2
}

ggarrange(raincloud_list[[1]],raincloud_list[[2]],raincloud_list[[3]],
          raincloud_list[[4]],raincloud_list[[5]],raincloud_list[[6]],
          raincloud_list[[7]],raincloud_list[[8]],raincloud_list[[9]],
          raincloud_list[[10]],ncol=3, nrow=4)

```

```{r raincloud plots - scaled vars}

# scale data and ensure vars are numeric and drop na (=select complete observations)
model_data_scale <- mdl_data_pooled %>%
  mutate_at(vars(runoff_racmo,salt_shallow,temp_shallow,salt_deep,temp_deep,ice_cover_percent,
                 velocity,glacier_len_norm,DOY),as.numeric) %>%
  mutate_at(vars(runoff_racmo,salt_shallow,temp_shallow,salt_deep,temp_deep,ice_cover_percent,
                 velocity,glacier_len_norm,DOY),scale)

model_data_scale <- as.data.frame(mdl_data_pooled) %>%
  mutate_at(vars(runoff_racmo,salt_shallow,temp_shallow,salt_deep,temp_deep,ice_cover_percent,
                 velocity,glacier_len_norm,DOY),scale)

variables <- c("DOY","runoff_racmo","salt_shallow","salt_deep","temp_shallow","temp_deep",
               "ice_cover_percent","velocity","noise_4kHz","glacier_len_norm")

raincloud_list <- list()

for (i in 1:length(variables)){
  data_1 <- model_data_scale %>% filter(narwhal==1) %>% dplyr::select(variables[i])
  data_2 <- model_data_scale %>% filter(narwhal==0) %>% dplyr::select(variables[i])
  
  # make dataframe
  df_1x1 <- data_1x1(
    array_1 = unlist(array(data_1[variables[i]])),
    array_2 = unlist(array(data_2[variables[i]])),
    jit_distance = .09,
    jit_seed = 321)
  
  # plot
  raincloud_2 <- raincloud_1x1_repmes(
    data = df_1x1,
    colors = (c('dodgerblue', 'darkorange')),
    fills = (c('dodgerblue', 'darkorange')),
    line_color = 'white',
    line_alpha = .3,
    size = 1,
    alpha = .6,
    align_clouds = FALSE) +
   
  scale_x_continuous(breaks=c(1,2), labels=c("1", "0"), limits=c(0, 3)) +
    xlab("Narwhal") + 
    ylab(variables[i]) +
    theme_classic()
  
  raincloud_list[[i]] <- raincloud_2
}

```

## Run models

```{r global glm}
## logistic regression - glm with binomial response and logit link 
## make sure year is coded as a factor
mdl_data_clean <- mdl_data_clean %>% mutate_at(vars(year), list(factor))

# DOY is negatively correlated with salt_deep (dropping salt_deep)
# temp_deep and salt_shallow are correlated (dropping salt_shallow)
# removed sea ice because it is zero for all 2019 data

## run global model without sea ice
mdl_gbl <- glm(narwhal ~ 
                 runoff_racmo+
                 glacier_len_norm+
                 noise_4kHz+
                 year+
                 site+
                 temp_shallow+
                 temp_deep+
                 velocity+
                 DOY+
                 DOY:runoff_racmo+
                 DOY:temp_shallow+
                 DOY:temp_deep,
               data = mdl_data_clean,
               family=binomial(link="logit"),
               na.action="na.fail")

# print summary output
summary(mdl_gbl)
faraway::sumary(mdl_gbl)

# run all possible model combinations
dredge_mdl <- dredge(mdl_gbl)
head(dredge_mdl)

# export table to csv
write.csv(as.data.frame(dredge_mdl), row.names = FALSE,
          here("analysis/csv_outputs/mdl_AIC_results.csv"))

```

```{r run best model}
## run global model without sea ice
mdl_best <- glm(narwhal ~ 
                 runoff_racmo+
                 year+
                 DOY,
               data = mdl_data_clean,
               family=binomial(link="logit"),
               na.action="na.fail")

# print summary output
summary(mdl_best)
faraway::sumary(mdl_best)

# check vif for collinearity
round(car::vif(mdl_best),3)

## get confidence intervals
round(confint(mdl_best),3)

```

## Exploratory plots for noise and sea ice data

```{r plot noise}
mdl_data_clean %>% ungroup() %>% filter(year==2018) %>% 
  dplyr::select(site,noise_4kHz,DOY,time) %>% 
  ggplot(aes(as.Date(time),noise_4kHz,color=site)) +
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("ambient noise (dB re 1 uPa)")+
  xlab("time")+
  ggtitle("2018 Ambient Noise")+
  ylim(80,110)+
  xlim(as.Date("2018-08-01"),as.Date("2018-11-01"))

mdl_data_clean %>% ungroup() %>% filter(year==2019) %>% 
  dplyr::select(site,noise_4kHz,DOY,time) %>% 
  ggplot(aes(as.Date(time),noise_4kHz,color=site)) +
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("ambient noise (dB re 1 uPa)")+
  xlab("time")+
  ggtitle("2019 Ambient Noise")+
  ylim(80,110)+
  xlim(as.Date("2019-08-01"),as.Date("2019-11-01"))
  
## do same plots for sea ice
mdl_data_clean %>% ungroup() %>% filter(year==2018) %>% 
  dplyr::select(site,ice_cover_percent,DOY,time) %>% 
  ggplot(aes(as.Date(time),ice_cover_percent,color=site)) +
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("ice cover (%)")+
  xlab("time")+
  ylim(0,100)+
  ggtitle("2018 Sea Ice")+
  xlim(as.Date("2018-08-01"),as.Date("2018-11-01"))

mdl_data_clean %>% ungroup() %>% filter(year==2019) %>% 
  dplyr::select(site,ice_cover_percent,DOY,time) %>% 
  ggplot(aes(as.Date(time),ice_cover_percent,color=site)) +
  geom_point()+
  geom_line()+
  theme_bw()+
  ylab("ice cover (%)")+
  xlab("time")+
  ggtitle("2019 Sea Ice")+
  ylim(0,100)+
  xlim(as.Date("2019-08-01"),as.Date("2019-11-01"))

```

```{r investigate ranges in temp and salinity}
# temperature
min(mdl_data_clean$temp_shallow)
min(mdl_data_clean$temp_deep)

max(mdl_data_clean$temp_shallow)
max(mdl_data_clean$temp_deep)

# salinity
min(mdl_data_clean$salt_shallow)
min(mdl_data_clean$salt_deep)

max(mdl_data_clean$salt_shallow)
max(mdl_data_clean$salt_deep)

```


```{r plot logistic regression result with observations}
## runoff ---------------------------------------
## plot narwhal prob vs sea ice with confidence intervals
## get fitted values
newdata18 <- data.frame(runoff_racmo = seq(0, 400),
                      DOY = rep(mean(mdl_data_clean$DOY),length(seq(0, 400))),
                      year = as.factor(rep(2018,length(seq(0, 400)))))
newdata19 <- data.frame(runoff_racmo = seq(0, 400),
                      DOY = rep(mean(mdl_data_clean$DOY),length(seq(0, 400))),
                      year = as.factor(rep(2019,length(seq(0, 400)))))

eta18 <- predict(mdl_best, newdata18)
eta19 <- predict(mdl_best, newdata19)
# transform out of logit space
p_hat18 <- 1 / (1 + exp(-eta18))
p_hat19 <- 1 / (1 + exp(-eta19))

## get the SE of the response
se18 <- predict(mdl_best, newdata18, type = "link", se.fit = TRUE)$se.fit
CI_upper18 <- 1 / (1 + exp(-(eta18 + 1.96*se18))) ## upper 95% CI
CI_lower18 <- 1 / (1 + exp(-(eta18 - 1.96*se18))) ## lower 95% CI

se19 <- predict(mdl_best, newdata19, type = "link", se.fit = TRUE)$se.fit
CI_upper19 <- 1 / (1 + exp(-(eta19 + 1.96*se19))) ## upper 95% CI
CI_lower19 <- 1 / (1 + exp(-(eta19 - 1.96*se19))) ## lower 95% CI

## 95% CI for beta_i based on profile likelihood
round(confint(mdl_best), 2)

## set plot area
par(mai = c(0.9, 0.9, 0.1, 0.1),
    omi = c(0, 0, 0, 0),
    cex.lab = 1)

## plot narwhal detections vs. ice cover
plot(narwhal ~ runoff_racmo, data = mdl_data_clean,
     pch = 16, cex = 1.3, col = 'black',
     yaxt = "n", ylab = "Narwhal prob (%)", xlab = "Runoff")
axis(2, at = c(0,0.5,1), labels = c("0","50", "100"), las = 1)

# ## add 95% CI's
runoff <- seq(0, 400)
polygon(c(runoff, rev(runoff)), c(CI_upper19, rev(CI_lower19)), border=NA,
        col = adjustcolor("orange",alpha.f=0.2))
polygon(c(runoff, rev(runoff)), c(CI_upper18, rev(CI_lower18)), border=NA,
        col = adjustcolor("gray",alpha.f=0.5))
## add model fit
lines(runoff, p_hat18, lwd = 2, col="darkgray")
lines(runoff, p_hat19, lwd = 2, col="orange")

## DOY ---------------------------------------
## plot narwhal prob vs DOY with confidence intervals
DOY <- seq(210,356)
## get fitted values
newdata18 <- data.frame(DOY = DOY,
                      runoff_racmo = rep(mean(mdl_data_clean$runoff_racmo),length(DOY)),
                      year = as.factor(rep(2018,length(DOY))))
newdata19 <- data.frame(DOY = DOY,
                      runoff_racmo = rep(mean(mdl_data_clean$runoff_racmo),length(DOY)),
                      year = as.factor(rep(2019,length(DOY))))

eta18 <- predict(mdl_best, newdata18)
eta19 <- predict(mdl_best, newdata19)
# transform out of logit space
p_hat18 <- 1 / (1 + exp(-eta18))
p_hat19 <- 1 / (1 + exp(-eta19))

## get the SE of the response
se18 <- predict(mdl_best, newdata18, type = "link", se.fit = TRUE)$se.fit
CI_upper18 <- 1 / (1 + exp(-(eta18 + 1.96*se18))) ## upper 95% CI
CI_lower18 <- 1 / (1 + exp(-(eta18 - 1.96*se18))) ## lower 95% CI

se19 <- predict(mdl_best, newdata19, type = "link", se.fit = TRUE)$se.fit
CI_upper19 <- 1 / (1 + exp(-(eta19 + 1.96*se19))) ## upper 95% CI
CI_lower19 <- 1 / (1 + exp(-(eta19 - 1.96*se19))) ## lower 95% CI

## plot narwhal detections vs. ice cover
plot(narwhal ~ DOY, data = mdl_data_clean,
     pch = 16, cex = 1.3, col = 'black',
     yaxt = "n", ylab = "Narwhal prob (%)", xlab = "DOY")
axis(2, at = c(0,0.5,1), labels = c("0","50", "100"), las = 1)

# ## add 95% CI's
polygon(c(DOY, rev(DOY)), c(CI_upper19, rev(CI_lower19)), border=NA,
        col = adjustcolor("orange",alpha.f=0.2))
polygon(c(DOY, rev(DOY)), c(CI_upper18, rev(CI_lower18)), border=NA,
        col = adjustcolor("gray",alpha.f=0.5))
## add model fit
lines(DOY, p_hat18, lwd = 2, col="darkgray")
lines(DOY, p_hat19, lwd = 2, col="orange")

```

## Diagnostics

```{r diagnostics}
## plot deviance residuals------------------------------------------
## set plot area
par(mai = c(0.9, 0.9, 0.1, 0.1),
omi = c(0, 0, 0, 0),
cex.lab = 1)
## plot resids vs eta
binnedplot(fitted(mdl_gbl), 
           residuals(mdl_gbl), las = 1, pch = 16,
           ylab = "Residuals", xlab = "Fitted values",main = "")

binnedplot(fitted(mdl_best), 
           residuals(mdl_best), las = 1, pch = 16,
           ylab = "Residuals", xlab = "Fitted values",main = "")

## leverages--------------------------------------------------------
## set plot area
par(mai = c(0.9, 1, 0.1, 0.1),
omi = c(0, 0, 0, 0),
cex.lab = 1)
levs <- hatvalues(mdl_best)
## threshhold value
h_crit <- 2 * length(coef(mdl_best)) / nn

## halfnormal plot
faraway::halfnorm(levs, las = 1, ylab = "")
text(0, 0.92*par("usr")[4], substitute(italic(h[crit]) == h_crit, 
                                       list(h_crit = h_crit)), pos = 4)
mtext(side = 2, text = "Sorted Data", line = 4)

## Cook's D---------------------------------------------------------
## set plot area
par(mai = c(0.9, 0.9, 0.1, 0.1),
omi = c(0, 0, 0, 0),
cex.lab = 1)
## halfnormal plot
CD <- cooks.distance(mdl_best)
faraway::halfnorm(CD, las = 1, ylab = "")
mtext(side = 2, text = "Sorted Data", line = 3.5)

## H-L test with 8 groups
# A non-significant p value indicates that there is no evidence that the observed and expected frequencies differ (i.e., evidence of good fit).
library(generalhoslem)
generalhoslem::logitgof(obs = mdl_data_clean$narwhal,
                        exp = fitted(mdl_best), g = 8)
```

```{r check autocorrelation, eval=FALSE}
acf(resid(mdl_best))
```

